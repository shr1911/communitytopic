{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "executionInfo": {
     "elapsed": 248,
     "status": "error",
     "timestamp": 1676068881668,
     "user": {
      "displayName": "Shraddha Mukesh Makwana",
      "userId": "09246782049398662435"
     },
     "user_tz": 420
    },
    "id": "qoox0zT4FEIy",
    "outputId": "77fef7d3-c28e-4a8a-e626-6f950d124580"
   },
   "outputs": [],
   "source": [
    "import network_creation\n",
    "import preprocessing\n",
    "import community_utils\n",
    "import tomotopy as tp\n",
    "import networkx as nx\n",
    "import igraph as ig\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5BkWXVfgFEIz"
   },
   "outputs": [],
   "source": [
    "with open(\"./text_datasets/20newsgroups_train.txt\", \"r\") as f:\n",
    "    bbc_train = f.read().split(\"\\n\")\n",
    "with open(\"./text_datasets/20newsgroups_test.txt\", \"r\") as f:\n",
    "    bbc_test = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NIukH0YSFEI0"
   },
   "outputs": [],
   "source": [
    "# create filter configuration dict\n",
    "filter_dict = {\"filter_short\": True,\n",
    "              \"filter_stopwords\": True,\n",
    "              \"filter_numbers\": True,\n",
    "              \"filter_punct\": True,\n",
    "              \"filter_websites\": True,\n",
    "              \"filter_emails\": True,\n",
    "              \"filter_not_wordlike\": True,\n",
    "              \"pos_filters\": [\"NOUN\", \"PROPN\"]}\n",
    "\n",
    "# create preprocessing pipeline\n",
    "nlp = preprocessing.create_pipeline(detect_sentences=True,\n",
    "                                    detect_entities=True,\n",
    "                                    entity_types=[\"EVENT\", \"FAC\", \"GPE\", \"LOC\", \"ORG\", \"PERSON\", \"PRODUCT\", \"WORK_OF_ART\"],\n",
    "                                    filter_config=filter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KmlRHXbPFEI0"
   },
   "outputs": [],
   "source": [
    "bbc_train_docs = list(nlp.pipe(bbc_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mjz3KyRNFEI0"
   },
   "outputs": [],
   "source": [
    "bbc_test_docs = list(nlp.pipe(bbc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Dnziw3-sFEI0"
   },
   "outputs": [],
   "source": [
    "tokenized_bbc_train_docs = list(preprocessing.tokenize_docs(bbc_train_docs, lowercase=True, sentences=False))\n",
    "tokenized_bbc_train_sents = list(preprocessing.tokenize_docs(bbc_train_docs, lowercase=True, sentences=True))\n",
    "tokenized_bbc_test_docs = list(preprocessing.tokenize_docs(bbc_test_docs, lowercase=True, sentences=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "f8fdvp94FEI1"
   },
   "outputs": [],
   "source": [
    "bbc_phrases, tokenized_bbc_train_docs, bbc_phrase_models = preprocessing.detect_phrases(tokenized_bbc_train_docs,\n",
    "                                                      num_iterations=2,\n",
    "                                                      scoring_method='npmi',\n",
    "                                                      threshold=0.35,\n",
    "                                                      min_count=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0RWlqtZRFEI1"
   },
   "outputs": [],
   "source": [
    "for model in bbc_phrase_models:\n",
    "    tokenized_bbc_train_sents = model[tokenized_bbc_train_sents]\n",
    "tokenized_bbc_train_sents = [[token.replace(\" \", \"_\") for token in sent] for sent in tokenized_bbc_train_sents]\n",
    "\n",
    "for model in bbc_phrase_models:\n",
    "    tokenized_bbc_test_docs = model[tokenized_bbc_test_docs]\n",
    "tokenized_bbc_test_docs = [[token.replace(\" \", \"_\") for token in doc] for doc in tokenized_bbc_test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-W_uRMp8FEI1"
   },
   "outputs": [],
   "source": [
    "bbc_vocab, bbc_dictionary = preprocessing.create_vocabulary_and_dictionary(tokenized_bbc_train_docs, min_threshold=None)\n",
    "tokenized_bbc_train_sents = preprocessing.filter_tokenized_docs_with_vocab(tokenized_bbc_train_sents, bbc_vocab)\n",
    "tokenized_bbc_train_docs = preprocessing.filter_tokenized_docs_with_vocab(tokenized_bbc_train_docs, bbc_vocab)\n",
    "tokenized_bbc_test_docs = preprocessing.filter_tokenized_docs_with_vocab(tokenized_bbc_test_docs, bbc_vocab)\n",
    "test_vocab = set()\n",
    "for doc in tokenized_bbc_test_docs:\n",
    "    for token in doc:\n",
    "        test_vocab.add(token)\n",
    "tokenized_bbc_train_docs = [[token for token in doc if token in test_vocab] for doc in tokenized_bbc_train_docs]\n",
    "tokenized_bbc_train_sents = [[token for token in sent if token in test_vocab] for sent in tokenized_bbc_train_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "kMMLCveGFEI1"
   },
   "outputs": [],
   "source": [
    "tokenized_bbc_train_sents = [sent for sent in tokenized_bbc_train_sents if len(sent) > 0]\n",
    "tokenized_bbc_train_docs = [doc for doc in tokenized_bbc_train_docs if len(doc) > 0]\n",
    "tokenized_bbc_test_docs = [doc for doc in tokenized_bbc_test_docs if len(doc) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ItFHMDtoFEI1"
   },
   "outputs": [],
   "source": [
    "bbc_sentence_nb = network_creation.SentenceNetworkBuilder(tokenized_bbc_train_sents, \n",
    "                                                         bbc_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "asbsido2FEI2"
   },
   "outputs": [],
   "source": [
    "bbc_sentence_nb.save_network(f\"./hierarchy_networks/bbc_sentence_npmi_0.txt\", type=\"npmi\", threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "JSPJAiyAFEI2"
   },
   "outputs": [],
   "source": [
    "nx_g = nx.read_weighted_edgelist(f\"./hierarchy_networks/bbc_sentence_npmi_0.txt\")\n",
    "ig_g = ig.Graph.from_networkx(nx_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "VeihIJm5FEI2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shraddha Makwana\\AppData\\Local\\Temp\\ipykernel_14196\\2947770478.py:1: DeprecationWarning: resolution_parameter keyword argument is deprecated, use resolution=... instead\n",
      "  clustering = ig_g.community_leiden(objective_function='modularity', weights='weight', resolution_parameter=1)\n"
     ]
    }
   ],
   "source": [
    "clustering = ig_g.community_leiden(objective_function='modularity', weights='weight', resolution_parameter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "fKLAu0pAFEI2",
    "outputId": "43c24872-eb77-40da-d586-de6fd6aea4cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "H72FvsX-FEI3"
   },
   "outputs": [],
   "source": [
    "topics = [[int(ig_g.vs[node][\"_nx_name\"]) for node in comm] for comm in clustering if len(comm) > 2]\n",
    "for comm in topics:\n",
    "    c = [str(node) for node in comm]\n",
    "    comm.sort(key=lambda node: community_utils.get_internal_weighted_degree(node, c, nx_g), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full topics saved to topics.txt\n",
      "['addition', 'car', 'engine', 'model', 'production', 'floppies', 'functionality', 'computer', 'display', 'folks']\n",
      "['body', 'info', 'rest', 'base', 'cpu', 'procedure', 'speed', 'usage', 'advance', 'machine']\n",
      "['day', 'doors', 'thanks', 'access', 'dirt', 'duo', 'line', 'people', 'round', 'rumors']\n",
      "['door', 'history', 'mail', 'specs', 'sports', 'years', 'cards', 'clock', 'days', 'disk']\n",
      "['poll', 'finals', 'errors', 'tom', 'course', 'controller', 'sheet', 'example', 'references', 'air_force']\n"
     ]
    }
   ],
   "source": [
    "topics_test = [[bbc_dictionary[node] for node in comm] for comm in clustering]\n",
    "\n",
    "with open(\"./topics.txt\", \"w\") as f:\n",
    "    lines = []\n",
    "    for topic in topics_test:\n",
    "        line = \" \".join(topic)\n",
    "        lines.append(line)\n",
    "    f.write(\"\\n\".join(lines))\n",
    "\n",
    "print(\"Full topics saved to topics.txt\")\n",
    "for topic in topics_test:\n",
    "    print(topic[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "0In2sGt9FEI3"
   },
   "outputs": [],
   "source": [
    "def get_topic_phi(topic, nx_g, level_0):\n",
    "    c = [str(node) for node in topic]\n",
    "    phi = np.zeros((len(level_0),))\n",
    "    for i, v in enumerate(level_0):\n",
    "        if v in topic:\n",
    "            phi[i] = community_utils.get_internal_weighted_degree(v, c, nx_g)\n",
    "\n",
    "    return phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "cUFwZQ6tFEI3"
   },
   "outputs": [],
   "source": [
    "level_0 = [int(ig_g.vs[v.index][\"_nx_name\"]) for v in ig_g.vs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "_Hy0qc46FEI3",
    "outputId": "8e81e47d-245e-4569-ea6e-c8b7fa55fc59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2918"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(level_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "kz2tomIyFEI3"
   },
   "outputs": [],
   "source": [
    "phi_norm = np.array([bbc_sentence_nb.token_freqs[v] for v in level_0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "UA-S3Z4DFEI3"
   },
   "outputs": [],
   "source": [
    "phi_norm = phi_norm / phi_norm.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "aEBzymREFEI4",
    "outputId": "235be30d-785c-44c0-d8b5-83aeb23d1367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.7027301415617562\n"
     ]
    }
   ],
   "source": [
    "phis = []\n",
    "for cluster in topics:\n",
    "    phi = get_topic_phi(cluster, nx_g, level_0)\n",
    "    phi = phi / phi.sum()\n",
    "    cos = np.dot(phi, phi_norm) / (np.linalg.norm(phi) * np.linalg.norm(phi_norm))\n",
    "    phis.append(1 - cos)\n",
    "\n",
    "print()\n",
    "print(sum(phis) / len(phis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "1A0NbU8VFEI4",
    "outputId": "3ef5368e-63c4-4814-bd1b-6a675708437d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 sub clusters found\n",
      "5 sub clusters found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shraddha Makwana\\AppData\\Local\\Temp\\ipykernel_14196\\73823597.py:5: DeprecationWarning: resolution_parameter keyword argument is deprecated, use resolution=... instead\n",
      "  subclusters = ig_sg.community_leiden(objective_function='modularity', resolution_parameter=1.0, weights='weight')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 sub clusters found\n",
      "4 sub clusters found\n",
      "5 sub clusters found\n"
     ]
    }
   ],
   "source": [
    "subclusterings = []\n",
    "subtopics = []\n",
    "for cluster in clustering:\n",
    "    ig_sg = ig_g.subgraph(cluster)\n",
    "    subclusters = ig_sg.community_leiden(objective_function='modularity', resolution_parameter=1.0, weights='weight')\n",
    "    print(f'{len(subclusters)} sub clusters found')\n",
    "    subclusterings.append(subclusters)\n",
    "    stopics = [[int(ig_sg.vs[node][\"_nx_name\"]) for node in comm] for comm in subclusters if len(comm) > 2]\n",
    "    for comm in stopics:\n",
    "        c = [str(node) for node in comm]\n",
    "        comm.sort(key=lambda node: community_utils.get_internal_weighted_degree(node, c, nx_g), reverse=True)\n",
    "    subtopics.append(stopics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Yx4qDtRVFEI4",
    "outputId": "544ce154-2dd5-4be4-ed6a-ff42c920ebe7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg specialization\n",
      "0.8823317325837333\n"
     ]
    }
   ],
   "source": [
    "phis = []\n",
    "\n",
    "for stopics in subtopics:\n",
    "    for topic in stopics:\n",
    "        phi = get_topic_phi(topic, nx_g, level_0)\n",
    "        phi = phi / phi.sum()\n",
    "        cos = np.dot(phi, phi_norm) / (np.linalg.norm(phi) * np.linalg.norm(phi_norm))\n",
    "        phis.append(1 - cos)\n",
    "\n",
    "print('Avg specialization')\n",
    "print(sum(phis) / len(phis))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "7wxp3_4aFEI4",
    "outputId": "cc1cf194-93f1-4d39-bce7-c18be21f51f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42574841928636964\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "child_sims = []\n",
    "nonchild_sims = []\n",
    "\n",
    "for i, supertopic in enumerate(topics):\n",
    "    super_phi = get_topic_phi(supertopic, nx_g, level_0)\n",
    "    for j, stopics in enumerate(subtopics):\n",
    "        for subtopic in stopics:\n",
    "            sub_phi = get_topic_phi(subtopic, nx_g, level_0)\n",
    "            cos = np.dot(super_phi, sub_phi) / (np.linalg.norm(super_phi) * np.linalg.norm(sub_phi))\n",
    "            if i == j:\n",
    "                child_sims.append(cos)\n",
    "            else:\n",
    "                nonchild_sims.append(cos)\n",
    "\n",
    "avg_child_sims = sum(child_sims) / len(child_sims)\n",
    "avg_nonchild_sims = sum(nonchild_sims) / len(nonchild_sims)\n",
    "\n",
    "print(avg_child_sims)\n",
    "print(avg_nonchild_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "z7vZffIlFEI4",
    "outputId": "8beffe20-2e85-4b84-b203-9c848a479347"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_v: 0.6733591663950255\n",
      "c_npmi: 0.06175667902927657\n"
     ]
    }
   ],
   "source": [
    "eval_topics = []\n",
    "for super_topic in topics:\n",
    "    eval_topics.append([bbc_dictionary[node] for node in super_topic])\n",
    "for stopic in subtopics:\n",
    "    for sub_topic in stopic:\n",
    "        eval_topics.append([bbc_dictionary[node] for node in sub_topic])\n",
    "\n",
    "for coherence in [\"c_v\", \"c_npmi\"]:\n",
    "    cm = CoherenceModel(topics=eval_topics, texts=tokenized_bbc_test_docs, dictionary=bbc_dictionary, topn=5, coherence=coherence)\n",
    "    score = cm.get_coherence()\n",
    "    print(f'{coherence}: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "nJjo74cwFEI4"
   },
   "outputs": [],
   "source": [
    "hlda_model = tp.HLDAModel(depth=4)\n",
    "\n",
    "for doc in tokenized_bbc_train_docs:\n",
    "    hlda_model.add_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "bKBDp2-9FEI5",
    "outputId": "49ed9084-422e-48f7-d3d9-b87a8ddd835a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: #0\tLog-likelihood: -7.226701854306841\n",
      "Iteration: #10\tLog-likelihood: -7.101672536347698\n",
      "Iteration: #20\tLog-likelihood: -7.056578853720364\n",
      "Iteration: #30\tLog-likelihood: -7.028760030832465\n",
      "Iteration: #40\tLog-likelihood: -7.012713281833828\n",
      "Iteration: #50\tLog-likelihood: -6.995792854880585\n",
      "Iteration: #60\tLog-likelihood: -6.984036350502312\n",
      "Iteration: #70\tLog-likelihood: -6.975818179868433\n",
      "Iteration: #80\tLog-likelihood: -6.965583308008099\n",
      "Iteration: #90\tLog-likelihood: -6.9623869592894225\n"
     ]
    }
   ],
   "source": [
    "iterations = 10\n",
    "for i in range(0, 100, iterations):\n",
    "    hlda_model.train(iterations)\n",
    "    print('Iteration: #{}\\tLog-likelihood: {}'.format(i, hlda_model.ll_per_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "lE6ABlM2FEI5",
    "outputId": "678598c3-4fb2-49ac-b025-bbf447208e26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2207\n"
     ]
    }
   ],
   "source": [
    "total_topics = 0\n",
    "\n",
    "for k in range(hlda_model.k):\n",
    "    if hlda_model.is_live_topic(k):\n",
    "        total_topics += 1\n",
    "\n",
    "print(total_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "g9e6o53KFEI5",
    "outputId": "6934abd0-ea01-4700-d21c-157e6d86f713"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1794\n"
     ]
    }
   ],
   "source": [
    "total_topics = 0\n",
    "\n",
    "for k in range(hlda_model.k):\n",
    "    if hlda_model.is_live_topic(k):\n",
    "        if (hlda_model.num_docs_of_topic(k) > 2):\n",
    "            total_topics += 1\n",
    "\n",
    "print(total_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "c_ygr-9qFEI5",
    "outputId": "f02c125d-94c2-4ae6-a243-fed8e46a0a72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "256\n",
      "686\n",
      "1264\n"
     ]
    }
   ],
   "source": [
    "level_0_count = 0\n",
    "level_1_count = 0\n",
    "level_2_count = 0\n",
    "level_3_count = 0\n",
    "\n",
    "for k in range(hlda_model.k):\n",
    "    if hlda_model.is_live_topic(k):\n",
    "        if (hlda_model.level(k) == 0):\n",
    "            level_0_count += 1\n",
    "        elif hlda_model.level(k) == 1:\n",
    "            level_1_count += 1\n",
    "        elif hlda_model.level(k) == 2:\n",
    "            level_2_count += 1\n",
    "        else:\n",
    "            level_3_count += 1\n",
    "\n",
    "print(level_0_count)\n",
    "print(level_1_count)\n",
    "print(level_2_count)\n",
    "print(level_3_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "UTrJgZpaFEI5",
    "outputId": "8e38b0a0-6649-4539-cc39-eb49f80477eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "249\n",
      "588\n",
      "956\n"
     ]
    }
   ],
   "source": [
    "level_0_count = 0\n",
    "level_1_count = 0\n",
    "level_2_count = 0\n",
    "level_3_count = 0\n",
    "\n",
    "for k in range(hlda_model.k):\n",
    "    if hlda_model.is_live_topic(k):\n",
    "        if hlda_model.num_docs_of_topic(k) > 2:\n",
    "            if (hlda_model.level(k) == 0):\n",
    "                level_0_count += 1\n",
    "            elif hlda_model.level(k) == 1:\n",
    "                level_1_count += 1\n",
    "            elif hlda_model.level(k) == 2:\n",
    "                level_2_count += 1\n",
    "            else:\n",
    "                level_3_count += 1\n",
    "\n",
    "print(level_0_count)\n",
    "print(level_1_count)\n",
    "print(level_2_count)\n",
    "print(level_3_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "o8Sb0FGCFEI5",
    "outputId": "252ff4a4-a947-4e34-bb1f-aed4b3e9cce8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "weird_count = 0\n",
    "\n",
    "for k in range(hlda_model.k):\n",
    "    if hlda_model.is_live_topic(k):\n",
    "        if hlda_model.level(k) == 2:\n",
    "            num_docs = hlda_model.num_docs_of_topic(k)\n",
    "            parent = hlda_model.parent_topic(k)\n",
    "            num_parent_docs = hlda_model.num_docs_of_topic(parent)\n",
    "            if num_docs >= num_parent_docs:\n",
    "                weird_count += 1\n",
    "\n",
    "print(weird_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "WIxLbYsyFEI5"
   },
   "outputs": [],
   "source": [
    "def get_hlda_phi(mdl, topic, dictionary, level_0):\n",
    "    phi = np.zeros((len(level_0),))\n",
    "    for term, prob in mdl.get_topic_words(topic, top_n=len(level_0)):\n",
    "        try:\n",
    "            term_id = dictionary.token2id[term]\n",
    "            phi_idx = level_0.index(term_id)\n",
    "            phi[phi_idx] = prob\n",
    "        except:\n",
    "            continue\n",
    "    return phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "9dr5uVGAFEI6",
    "outputId": "0b82dc3c-aab4-4c27-d3c3-7a7ccb3cce8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 1 specialization:  0.8816888150781839\n",
      "level 2 specialization:  0.8806667016460439\n",
      "level 3 specialization:  0.8731021859184818\n"
     ]
    }
   ],
   "source": [
    "level_1_specs = []\n",
    "level_2_specs = []\n",
    "level_3_specs = []\n",
    "\n",
    "for k in range(hlda_model.k):\n",
    "    if hlda_model.is_live_topic(k):\n",
    "        if hlda_model.level(k) == 1:\n",
    "            phi = get_hlda_phi(hlda_model, k, bbc_dictionary, level_0)\n",
    "            cos = np.dot(phi_norm, phi) / (np.linalg.norm(phi_norm) * np.linalg.norm(phi))\n",
    "            level_1_specs.append(1 - cos)\n",
    "        elif hlda_model.level(k) == 2:\n",
    "            phi = get_hlda_phi(hlda_model, k, bbc_dictionary, level_0)\n",
    "            cos = np.dot(phi_norm, phi) / (np.linalg.norm(phi_norm) * np.linalg.norm(phi))\n",
    "            level_2_specs.append(1 - cos)\n",
    "        elif hlda_model.level(k) == 3:\n",
    "            phi = get_hlda_phi(hlda_model, k, bbc_dictionary, level_0)\n",
    "            cos = np.dot(phi_norm, phi) / (np.linalg.norm(phi_norm) * np.linalg.norm(phi))\n",
    "            level_3_specs.append(1 - cos)\n",
    "\n",
    "print(\"level 1 specialization: \", sum(level_1_specs) / len(level_1_specs))\n",
    "print(\"level 2 specialization: \", sum(level_2_specs) / len(level_2_specs))\n",
    "print(\"level 3 specialization: \", sum(level_3_specs) / len(level_3_specs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "glRTkDk3FEI6"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "super_phis = dict()\n",
    "sub_phis = defaultdict(list)\n",
    "\n",
    "for k in range(hlda_model.k):\n",
    "    if hlda_model.is_live_topic(k):\n",
    "        if hlda_model.level(k) == 1:\n",
    "            phi = get_hlda_phi(hlda_model, k, bbc_dictionary, level_0)\n",
    "            super_phis[k] = phi\n",
    "        elif hlda_model.level(k) == 2:\n",
    "            phi = get_hlda_phi(hlda_model, k, bbc_dictionary, level_0)\n",
    "            parent = hlda_model.parent_topic(k)\n",
    "            sub_phis[parent].append(phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Y0wzfp11FEI6",
    "outputId": "3af95f4e-0e12-4e83-9587-429efc3b0199"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hierarchical Affinity level 1 and 2\n",
      "0.04405323992527675 (child)\n",
      "0.019209528441067655 non-child\n"
     ]
    }
   ],
   "source": [
    "child_sims = []\n",
    "nonchild_sims = []\n",
    "\n",
    "for parent, parent_phi in super_phis.items():\n",
    "    for p, cs in sub_phis.items():\n",
    "        for c_phi in cs:\n",
    "            cos = np.dot(parent_phi, c_phi) / (np.linalg.norm(parent_phi) * np.linalg.norm(c_phi))\n",
    "            if parent == p:\n",
    "                child_sims.append(cos)\n",
    "            else:\n",
    "                nonchild_sims.append(cos)\n",
    "                \n",
    "print(\"Hierarchical Affinity level 1 and 2\")\n",
    "print(sum(child_sims) / len(child_sims), \"(child)\")\n",
    "print(sum(nonchild_sims) / len(nonchild_sims), \"non-child\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "UOfYVvYYFEI6"
   },
   "outputs": [],
   "source": [
    "eval_topics = []\n",
    "for k in range(hlda_model.k):\n",
    "    if hlda_model.is_live_topic(k):\n",
    "        eval_topics.append([w for w, _ in hlda_model.get_topic_words(k, top_n=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "WN2pdy6qFEI6",
    "outputId": "d8eaf74d-e9f0-4ba9-bca4-c96955cf1a94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_v: 0.4276809630300829\n",
      "c_npmi: -0.14553906538077566\n"
     ]
    }
   ],
   "source": [
    "for coherence in [\"c_v\", \"c_npmi\"]:\n",
    "    cm = CoherenceModel(topics=eval_topics, texts=tokenized_bbc_test_docs, dictionary=bbc_dictionary, topn=5, coherence=coherence)\n",
    "    score = cm.get_coherence()\n",
    "    print(f'{coherence}: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "0TrKzEMzFEI6"
   },
   "outputs": [],
   "source": [
    "pam_model = tp.PAModel(k1=5, k2=30)\n",
    "\n",
    "for doc in tokenized_bbc_train_docs:\n",
    "    pam_model.add_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "Cfv1gXyeFEI6",
    "outputId": "b73cba6e-2575-4f47-c4de-44b3ccda9afb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: #0\tLog-likelihood: -10.551583517064913\n",
      "Iteration: #10\tLog-likelihood: -10.00039035928291\n",
      "Iteration: #20\tLog-likelihood: -9.888507715387012\n",
      "Iteration: #30\tLog-likelihood: -9.829247080263059\n",
      "Iteration: #40\tLog-likelihood: -9.807981821209532\n",
      "Iteration: #50\tLog-likelihood: -9.785678914264812\n",
      "Iteration: #60\tLog-likelihood: -9.779550593345256\n",
      "Iteration: #70\tLog-likelihood: -9.765294201453706\n",
      "Iteration: #80\tLog-likelihood: -9.762189594318853\n",
      "Iteration: #90\tLog-likelihood: -9.74867736840671\n"
     ]
    }
   ],
   "source": [
    "iterations = 10\n",
    "for i in range(0, 100, iterations):\n",
    "    pam_model.train(iterations)\n",
    "    print('Iteration: #{}\\tLog-likelihood: {}'.format(i, pam_model.ll_per_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "T37hrMcwFEI6"
   },
   "outputs": [],
   "source": [
    "super_topic_phis = []\n",
    "sub_topic_phis = defaultdict(list)\n",
    "for k in range(pam_model.k1):\n",
    "    super_phi = np.zeros((len(level_0),))\n",
    "    i = 0\n",
    "    for subtopic, prob in pam_model.get_sub_topics(k, top_n=30):\n",
    "        sub_phi = get_hlda_phi(pam_model, subtopic, bbc_dictionary, level_0)\n",
    "        if i < 6:\n",
    "            sub_topic_phis[k].append(sub_phi)\n",
    "        super_phi += (prob * sub_phi)\n",
    "        i += 1\n",
    "    super_topic_phis.append(super_phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "24wnxH2dFEI7",
    "outputId": "90059afc-b6de-4ff6-a683-d06f6b38a71f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005602825395026411\n",
      "0.559521097401565\n"
     ]
    }
   ],
   "source": [
    "level_1_specs = []\n",
    "level_2_specs = []\n",
    "\n",
    "for k in range(pam_model.k1):\n",
    "    super_phi = super_topic_phis[k]\n",
    "    cos = np.dot(phi_norm, super_phi) / (np.linalg.norm(phi_norm) * np.linalg.norm(super_phi))\n",
    "    level_1_specs.append(1 - cos)\n",
    "    for sub_phi in sub_topic_phis[k]:\n",
    "        cos = np.dot(phi_norm, sub_phi) / (np.linalg.norm(phi_norm) * np.linalg.norm(sub_phi))\n",
    "        level_2_specs.append(1 - cos)\n",
    "\n",
    "print(sum(level_1_specs) / len(level_1_specs))\n",
    "print(sum(level_2_specs) / len(level_2_specs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "sDcZEk90FEI7",
    "outputId": "d4d5361f-81d2-4e7e-f657-dc4a211b7205"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45112651482523425\n",
      "0.4349652665672422\n"
     ]
    }
   ],
   "source": [
    "child_sims = []\n",
    "nonchild_sims = []\n",
    "\n",
    "for parent, parent_phi in enumerate(super_topic_phis):\n",
    "    for p, cs in sub_topic_phis.items():\n",
    "        for c_phi in cs:\n",
    "            cos = np.dot(parent_phi, c_phi) / (np.linalg.norm(parent_phi) * np.linalg.norm(c_phi))\n",
    "            if parent == p:\n",
    "                child_sims.append(cos)\n",
    "            else:\n",
    "                nonchild_sims.append(cos)\n",
    "\n",
    "print(sum(child_sims) / len(child_sims))\n",
    "print(sum(nonchild_sims) / len(nonchild_sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "zl-r3OsYFEI7"
   },
   "outputs": [],
   "source": [
    "eval_topics = []\n",
    "for k in range(pam_model.k1):\n",
    "    t = []\n",
    "    for idx in np.argpartition(super_topic_phis[0], -5)[-5:]:\n",
    "        t.append(bbc_dictionary[level_0[idx]])\n",
    "    eval_topics.append(t)\n",
    "for k in range(pam_model.k1, pam_model.k2):\n",
    "    eval_topics.append([w for w, _ in pam_model.get_topic_words(k, top_n=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "69PDlytrFEI7",
    "outputId": "9ac9f8c4-bfcc-41b4-c9f7-cdb4a7cbbd76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_v: 0.6516657781153187\n",
      "c_npmi: 0.11427878851283484\n"
     ]
    }
   ],
   "source": [
    "for coherence in [\"c_v\", \"c_npmi\"]:\n",
    "    cm = CoherenceModel(topics=eval_topics, texts=tokenized_bbc_test_docs, dictionary=bbc_dictionary, topn=5, coherence=coherence)\n",
    "    score = cm.get_coherence()\n",
    "    print(f'{coherence}: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tried new one HAPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpam_model = tp.HPAModel(k1=5, k2=30)\n",
    "\n",
    "for doc in tokenized_bbc_train_docs:\n",
    "    hpam_model.add_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: #0\tLog-likelihood: -9.728332805512178\n",
      "Iteration: #10\tLog-likelihood: -9.346376235485627\n",
      "Iteration: #20\tLog-likelihood: -9.28974521167038\n",
      "Iteration: #30\tLog-likelihood: -9.26071704907\n",
      "Iteration: #40\tLog-likelihood: -9.262398603937344\n",
      "Iteration: #50\tLog-likelihood: -9.266059680329985\n",
      "Iteration: #60\tLog-likelihood: -9.267216800737721\n",
      "Iteration: #70\tLog-likelihood: -9.267400697321865\n",
      "Iteration: #80\tLog-likelihood: -9.27514962583069\n",
      "Iteration: #90\tLog-likelihood: -9.283231945593016\n"
     ]
    }
   ],
   "source": [
    "iterations = 10\n",
    "for i in range(0, 100, iterations):\n",
    "    hpam_model.train(iterations)\n",
    "    print('Iteration: #{}\\tLog-likelihood: {}'.format(i, hpam_model.ll_per_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_topic_phis = []\n",
    "sub_topic_phis = defaultdict(list)\n",
    "for k in range(hpam_model.k1):\n",
    "    super_phi = np.zeros((len(level_0),))\n",
    "    i = 0\n",
    "    for subtopic, prob in hpam_model.get_sub_topics(k, top_n=30):\n",
    "        sub_phi = get_hlda_phi(hpam_model, subtopic, bbc_dictionary, level_0)\n",
    "        if i < 6:\n",
    "            sub_topic_phis[k].append(sub_phi)\n",
    "        super_phi += (prob * sub_phi)\n",
    "        i += 1\n",
    "    super_topic_phis.append(super_phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09631363903023363\n",
      "0.7198176398359268\n"
     ]
    }
   ],
   "source": [
    "level_1_specs = []\n",
    "level_2_specs = []\n",
    "\n",
    "for k in range(hpam_model.k1):\n",
    "    super_phi = super_topic_phis[k]\n",
    "    cos = np.dot(phi_norm, super_phi) / (np.linalg.norm(phi_norm) * np.linalg.norm(super_phi))\n",
    "    level_1_specs.append(1 - cos)\n",
    "    for sub_phi in sub_topic_phis[k]:\n",
    "        cos = np.dot(phi_norm, sub_phi) / (np.linalg.norm(phi_norm) * np.linalg.norm(sub_phi))\n",
    "        level_2_specs.append(1 - cos)\n",
    "\n",
    "print(sum(level_1_specs) / len(level_1_specs))\n",
    "print(sum(level_2_specs) / len(level_2_specs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3631621852597141\n",
      "0.330285608273151\n"
     ]
    }
   ],
   "source": [
    "child_sims = []\n",
    "nonchild_sims = []\n",
    "\n",
    "for parent, parent_phi in enumerate(super_topic_phis):\n",
    "    for p, cs in sub_topic_phis.items():\n",
    "        for c_phi in cs:\n",
    "            cos = np.dot(parent_phi, c_phi) / (np.linalg.norm(parent_phi) * np.linalg.norm(c_phi))\n",
    "            if parent == p:\n",
    "                child_sims.append(cos)\n",
    "            else:\n",
    "                nonchild_sims.append(cos)\n",
    "\n",
    "print(sum(child_sims) / len(child_sims))\n",
    "print(sum(nonchild_sims) / len(nonchild_sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_topics = []\n",
    "for k in range(hpam_model.k1):\n",
    "    t = []\n",
    "    for idx in np.argpartition(super_topic_phis[0], -5)[-5:]:\n",
    "        t.append(bbc_dictionary[level_0[idx]])\n",
    "    eval_topics.append(t)\n",
    "for k in range(hpam_model.k1, hpam_model.k2):\n",
    "    eval_topics.append([w for w, _ in hpam_model.get_topic_words(k, top_n=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_v: 0.6320137051245874\n",
      "c_npmi: 0.08799161208109782\n"
     ]
    }
   ],
   "source": [
    "for coherence in [\"c_v\", \"c_npmi\"]:\n",
    "    cm = CoherenceModel(topics=eval_topics, texts=tokenized_bbc_test_docs, dictionary=bbc_dictionary, topn=5, coherence=coherence)\n",
    "    score = cm.get_coherence()\n",
    "    print(f'{coherence}: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "interpreter": {
   "hash": "23700de4ded308992cfe54f74a91fcd6992a435e9e8f9388c14d9c44c42fa073"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
