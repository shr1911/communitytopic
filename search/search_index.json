{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Community Topic\u2019s documentation! Introduction What is Community Topic ? In this repository we present our novel method called Community Topic for Topic Modelleling as a Pypi library. Our algorithm, Community Topic, is based on mining communities of terms from term-occurrence networks extracted from the documents. In addition to providing interpretable collections of terms as topics, the network representation provides a natural topic structure. The topics form a network, so topic similarity is inferred from the weights of the edges between them. Super-topics can be found by iteratively applying community detection on the topic network, grouping similar topics together. Sub-topics can be found by iteratively applying community detection on a single topic community. This can be done dynamically, with the user or conversation agent moving up and down the topic hierarchy as desired. What problem does it solve? & Who is it for? Unfortunately, the most popular topic models in use today do not provide a suitable topic structure for these purposes and the state-of-the-art models based on neural networks suffer from many of the same drawbacks while requiring specialized hardware and many hours to train. This makes Community Topic an ideal topic modelling algorithm for both applied research and practical applications like conversational agents. Content Installation Guide Getting Started API usage","title":"Home"},{"location":"#welcome-to-community-topics-documentation","text":"","title":"Welcome to Community Topic\u2019s documentation!"},{"location":"#introduction","text":"What is Community Topic ? In this repository we present our novel method called Community Topic for Topic Modelleling as a Pypi library. Our algorithm, Community Topic, is based on mining communities of terms from term-occurrence networks extracted from the documents. In addition to providing interpretable collections of terms as topics, the network representation provides a natural topic structure. The topics form a network, so topic similarity is inferred from the weights of the edges between them. Super-topics can be found by iteratively applying community detection on the topic network, grouping similar topics together. Sub-topics can be found by iteratively applying community detection on a single topic community. This can be done dynamically, with the user or conversation agent moving up and down the topic hierarchy as desired. What problem does it solve? & Who is it for? Unfortunately, the most popular topic models in use today do not provide a suitable topic structure for these purposes and the state-of-the-art models based on neural networks suffer from many of the same drawbacks while requiring specialized hardware and many hours to train. This makes Community Topic an ideal topic modelling algorithm for both applied research and practical applications like conversational agents.","title":"Introduction"},{"location":"#_1","text":"","title":""},{"location":"#content","text":"Installation Guide Getting Started API usage","title":"Content"},{"location":"api-reference/","text":"API Usage There are two important classes in the library as follows:- - PreProcessing - CommunityTopic We will see API usage of both of them. Pre-Processing Module Method: do_preprocessing The do_preprocessing method performs pre-processing on a given training and testing corpus to convert it into a format suitable for CommunityTopic . Parameters do_preprocessing(train=None, test=None, ner=1, pos_filter=0, phrases='npmi', phrase_threshold=0.35, language='en') train : str Input training corpus test : str Input testing corpus ner : int Named Entity Recognition flag Possible values = [0, 1] 0 - to not use NER 1 - to use NER pos_filter : int Part-of-Speech filter for extracting features and marking the words in a text with labels for entity extraction Possible values = [0, 1, 2, 3] 0 - No POS filtering 1 - Keep only adjectives, adverbs, nouns, proper nouns, and verbs 2 - Keep only adjectives, nouns, proper nouns 3 - Keep only nouns, proper nouns phrases : str Currently using 'npmi' type for phrase detection phrase_threshold : float Phrase detection threshold Currently using 0.35 language : str Possible values = ['en', 'it', 'fr', 'de', 'es'] 'en' - English 'it' - Italian 'fr' - French 'de' - German 'es' - Spanish Language of the training and testing corpus Returns tokenized_train_sents tokenized_train_docs tokenized_test_docs dictionary tokenized_train_sents : list of list Returns pre-processed training corpus as sentences (in list of words form) tokenized_train_docs : list of list Returns pre-processed training corpus as docs (in list of words form) tokenized_test_docs : list of list Returns pre-processed testing corpus as sentences (in list of words form) dictionary : dict Gensim dictionary object that tracks frequencies and can filter vocab Keys are id for words Values are words Community Topic Module Class constructor: __init__ __init__(self, train_corpus=None, dictionary=None, edge_weight='count', weight_threshold=0.0, cd_algorithm='leiden', resolution_parameter=1.0, network_window='sentence') Parameters train_corpus : list of list (of string) Preprocessed sentences of training corpus (List of list) It contains pre-processed tokenized sentence as list of list dictionary : dict Gensim dictionary object that tracks frequencies and can filter vocab keys are id for words values are words edge_weight : str It is weight of edges which comes from the frequency of co-occurrence. Possible values: [\"count\", \"npmi\"] \"count\": Raw count of possible edges as the edge weight. \"npmi\": Weighing scheme which uses Normalized Pointwise Mutual Information (NPMI) between terms weight_threshold : float The edges can be thresholded, i.e. those edges whose weights fall below a certain threshold are removed from the network. cd_algorithm : str To choose community detection algorithm, possible values: [\"leiden\", \"walktrap\"] resolution_parameter : float Te resolution_parameter to use for leiden community detection algorithm. Higher resolution_parameter lead to smaller communities, while lower resolution_parameter lead to fewer larger communities. network_window : The network that we construct from a corpus has terms as vertices. This decides the fixed sliding window of document. Possible values: [\"sentence\", \"5\", \"10\"] \"sentence\": two terms co-occur if they both occur in the same sentence. \"5\" or \"10\": two terms co-occur if they both occur within a fixed-size sliding window over a document. Method: fit fit() This method performs task of finding simple topics Method: fit_hierarchical fit_hierarchical(n_level=2) This method performs task of finding hierarchical topics Parameter n_level : int Number of level for hierarchical topics Method: get_topics_words get_topics_words() Get topic words of flat topic modelling Returns topics : list of list Returns flat topics as topic words Method: get_topics_words_topn get_topics_words_topn(n=10) Get top n topic words of flat topic modelling Parameter n : int top n topic words Returns topics : list of list Returns top n flat topics as topic words Method: get_topics get_topics() Get topic as dictionary id Returns topics : list of list Returns flat topics as dictionary id Method: get_topic_words_hierarchical get_topic_words_hierarchical() Get hierarchical topic as topic words Returns hierarchical_topics_words : dict of dict In following format (each level and topic in that level)- { 1 : {\"0\": ['firm', 'company', 'economy',...], \"1\": ['country', 'china', 'bank'....], } .....}, 2 : {\"0\": [''orders', 'spring', 'allies',...], \"1\": ['lawyer', 'individuals', 'failure'....], } .....}, ..... } Method: get_topics_hierarchical get_topics_hierarchical() Get hierarchical topic as dictionary id, and ig_graph of topic Parameter n : int top n topic words Returns hierarchical_topics : dict of dict Returns top hierarchical topics as topic words In following format (each level and topic in that level)- { 1 : {\"0\": {'dict_num': [2, 147, 6, 1180, 327, ,....], 'ig_graph': object of ig_graph }, \"1\": {'dict_num': [2, 147, 6, 1180, 327, ,....], 'ig_graph': object of ig_graph }, .....}, 2 : {\"0_0\": {'dict_num': [2, 147, 6, 1180, 327, ,....], 'ig_graph': object of ig_graph}, \"0_1\": {'dict_num': [2, 147, 6, 1180, 327, ,....], 'ig_graph': object of ig_graph}, ..... ..... \"1_0\": {'dict_num': [2, 147, 6, 1180, 327, ,....], 'ig_graph': object of ig_graph}, \"1_1\": {'dict_num': [2, 147, 6, 1180, 327, ,....], 'ig_graph': object of ig_graph}, .....}, 3 : {\"0_0_0\": {'dict_num': [2, 147, 6, 1180, 327, ,....], 'ig_graph': object of ig_graph}, \"0_0_1\": {'dict_num': [2, 147, 6, 1180, 327, ,....], 'ig_graph': object of ig_graph}, ..... }, ...... } Note, in above format each level has topic names as the key of dictionary. For example, level 1 has single digit value which specifies topics in that level level 2 has two values seperated by underscore, first value is super topic and second is child topic Similary, level 3 has three values, for which parent topics and current child topic Method: get_topics_hierarchical get_n_level_topic_words_hierarchical(n_level=2) Get first n number of levels from hierarchy Parameter n_level : int top n level Returns topics : dict of dict Method: get_hierarchy_tree get_hierarchy_tree() This function is for visualisation purpose of hierarchical topics. Returns tree : It returns a tree-like structure in dictionary format.","title":"API Usage"},{"location":"api-reference/#api-usage","text":"There are two important classes in the library as follows:- - PreProcessing - CommunityTopic We will see API usage of both of them.","title":"API Usage"},{"location":"api-reference/#pre-processing-module","text":"","title":"Pre-Processing Module"},{"location":"api-reference/#method-do_preprocessing","text":"The do_preprocessing method performs pre-processing on a given training and testing corpus to convert it into a format suitable for CommunityTopic .","title":"Method: do_preprocessing"},{"location":"api-reference/#parameters","text":"do_preprocessing(train=None, test=None, ner=1, pos_filter=0, phrases='npmi', phrase_threshold=0.35, language='en') train : str Input training corpus test : str Input testing corpus ner : int Named Entity Recognition flag Possible values = [0, 1] 0 - to not use NER 1 - to use NER pos_filter : int Part-of-Speech filter for extracting features and marking the words in a text with labels for entity extraction Possible values = [0, 1, 2, 3] 0 - No POS filtering 1 - Keep only adjectives, adverbs, nouns, proper nouns, and verbs 2 - Keep only adjectives, nouns, proper nouns 3 - Keep only nouns, proper nouns phrases : str Currently using 'npmi' type for phrase detection phrase_threshold : float Phrase detection threshold Currently using 0.35 language : str Possible values = ['en', 'it', 'fr', 'de', 'es'] 'en' - English 'it' - Italian 'fr' - French 'de' - German 'es' - Spanish Language of the training and testing corpus","title":"Parameters"},{"location":"api-reference/#returns","text":"tokenized_train_sents tokenized_train_docs tokenized_test_docs dictionary tokenized_train_sents : list of list Returns pre-processed training corpus as sentences (in list of words form) tokenized_train_docs : list of list Returns pre-processed training corpus as docs (in list of words form) tokenized_test_docs : list of list Returns pre-processed testing corpus as sentences (in list of words form) dictionary : dict Gensim dictionary object that tracks frequencies and can filter vocab Keys are id for words Values are words","title":"Returns"},{"location":"api-reference/#community-topic-module","text":"","title":"Community Topic Module"},{"location":"api-reference/#class-constructor-__init__","text":"__init__(self, train_corpus=None, dictionary=None, edge_weight='count', weight_threshold=0.0, cd_algorithm='leiden', resolution_parameter=1.0, network_window='sentence')","title":"Class constructor: __init__ "},{"location":"api-reference/#parameters_1","text":"train_corpus : list of list (of string) Preprocessed sentences of training corpus (List of list) It contains pre-processed tokenized sentence as list of list dictionary : dict Gensim dictionary object that tracks frequencies and can filter vocab keys are id for words values are words edge_weight : str It is weight of edges which comes from the frequency of co-occurrence. Possible values: [\"count\", \"npmi\"] \"count\": Raw count of possible edges as the edge weight. \"npmi\": Weighing scheme which uses Normalized Pointwise Mutual Information (NPMI) between terms weight_threshold : float The edges can be thresholded, i.e. those edges whose weights fall below a certain threshold are removed from the network. cd_algorithm : str To choose community detection algorithm, possible values: [\"leiden\", \"walktrap\"] resolution_parameter : float Te resolution_parameter to use for leiden community detection algorithm. Higher resolution_parameter lead to smaller communities, while lower resolution_parameter lead to fewer larger communities. network_window : The network that we construct from a corpus has terms as vertices. This decides the fixed sliding window of document. Possible values: [\"sentence\", \"5\", \"10\"] \"sentence\": two terms co-occur if they both occur in the same sentence. \"5\" or \"10\": two terms co-occur if they both occur within a fixed-size sliding window over a document.","title":"Parameters"},{"location":"api-reference/#method-fit","text":"fit() This method performs task of finding simple topics","title":"Method: fit "},{"location":"api-reference/#method-fit_hierarchical","text":"fit_hierarchical(n_level=2) This method performs task of finding hierarchical topics","title":"Method: fit_hierarchical "},{"location":"api-reference/#parameter","text":"n_level : int Number of level for hierarchical topics","title":"Parameter"},{"location":"api-reference/#method-get_topics_words","text":"get_topics_words() Get topic words of flat topic modelling","title":"Method: get_topics_words "},{"location":"api-reference/#returns_1","text":"topics : list of list Returns flat topics as topic words","title":"Returns"},{"location":"api-reference/#method-get_topics_words_topn","text":"get_topics_words_topn(n=10) Get top n topic words of flat topic modelling","title":"Method: get_topics_words_topn "},{"location":"api-reference/#parameter_1","text":"n : int top n topic words","title":"Parameter"},{"location":"api-reference/#returns_2","text":"topics : list of list Returns top n flat topics as topic words","title":"Returns"},{"location":"api-reference/#method-get_topics","text":"get_topics() Get topic as dictionary id","title":"Method: get_topics "},{"location":"api-reference/#returns_3","text":"topics : list of list Returns flat topics as dictionary id","title":"Returns"},{"location":"api-reference/#method-get_topic_words_hierarchical","text":"get_topic_words_hierarchical() Get hierarchical topic as topic words","title":"Method: get_topic_words_hierarchical "},{"location":"api-reference/#returns_4","text":"hierarchical_topics_words : dict of dict In following format (each level and topic in that level)- { 1 : {\"0\": ['firm', 'company', 'economy',...], \"1\": ['country', 'china', 'bank'....], } .....}, 2 : {\"0\": [''orders', 'spring', 'allies',...], \"1\": ['lawyer', 'individuals', 'failure'....], } .....}, ..... }","title":"Returns"},{"location":"api-reference/#method-get_topics_hierarchical","text":"get_topics_hierarchical() Get hierarchical topic as dictionary id, and ig_graph of topic","title":"Method: get_topics_hierarchical "},{"location":"api-reference/#parameter_2","text":"n : int top n topic words","title":"Parameter"},{"location":"api-reference/#returns_5","text":"hierarchical_topics : dict of dict Returns top hierarchical topics as topic words In following format (each level and topic in that level)- { 1 : {\"0\": {'dict_num': [2, 147, 6, 1180, 327, ,....], 'ig_graph': object of ig_graph }, \"1\": {'dict_num': [2, 147, 6, 1180, 327, ,....], 'ig_graph': object of ig_graph }, .....}, 2 : {\"0_0\": {'dict_num': [2, 147, 6, 1180, 327, ,....], 'ig_graph': object of ig_graph}, \"0_1\": {'dict_num': [2, 147, 6, 1180, 327, ,....], 'ig_graph': object of ig_graph}, ..... ..... \"1_0\": {'dict_num': [2, 147, 6, 1180, 327, ,....], 'ig_graph': object of ig_graph}, \"1_1\": {'dict_num': [2, 147, 6, 1180, 327, ,....], 'ig_graph': object of ig_graph}, .....}, 3 : {\"0_0_0\": {'dict_num': [2, 147, 6, 1180, 327, ,....], 'ig_graph': object of ig_graph}, \"0_0_1\": {'dict_num': [2, 147, 6, 1180, 327, ,....], 'ig_graph': object of ig_graph}, ..... }, ...... } Note, in above format each level has topic names as the key of dictionary. For example, level 1 has single digit value which specifies topics in that level level 2 has two values seperated by underscore, first value is super topic and second is child topic Similary, level 3 has three values, for which parent topics and current child topic","title":"Returns"},{"location":"api-reference/#method-get_topics_hierarchical_1","text":"get_n_level_topic_words_hierarchical(n_level=2) Get first n number of levels from hierarchy","title":"Method: get_topics_hierarchical "},{"location":"api-reference/#parameter_3","text":"n_level : int top n level","title":"Parameter"},{"location":"api-reference/#returns_6","text":"topics : dict of dict","title":"Returns"},{"location":"api-reference/#method-get_hierarchy_tree","text":"get_hierarchy_tree() This function is for visualisation purpose of hierarchical topics.","title":"Method: get_hierarchy_tree "},{"location":"api-reference/#returns_7","text":"tree : It returns a tree-like structure in dictionary format.","title":"Returns"},{"location":"getting-started/","text":"Getting Started This is an example tuotrial which finds topic of BBC dataset using best combination for Pre-Processing and Community Topic Algorithm. Step 1: import necessary class of the library from communitytopic import CommunityTopic from communitytopic import PreProcessing Step 2: Load raw corpus as the dataset, here we are using BBC dataset. with open(\"<Path-To-train-Dataset>/bbc_train.txt\", \"r\", encoding='utf-8') as f: bbc_train = f.read() with open(\"<Path-To-Test-Dataset>/bbc_test.txt\", \"r\", encoding='utf-8') as f: bbc_test = f.read() Step 3: Performing pre-processing on training and testing corpus tokenized_bbc_train_sents, tokenized_bbc_train_docs, tokenized_bbc_test_docs, dictionary = PreProcessing.do_preprocessing( train=bbc_train, test=bbc_test, ner=1, pos_filter=3, phrases=\"npmi\", phrase_threshold=0.35, language=\"en\") Step 4: Applying Community Topic algorithm on pre-processed data community_topic = CommunityTopic(train_corpus=tokenized_bbc_train_sents, dictionary=dictionary) community_topic.fit() Step 5: Get topic words founded by abovr algorithm topic_words = community_topic.get_topics_words_topn(10)","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"This is an example tuotrial which finds topic of BBC dataset using best combination for Pre-Processing and Community Topic Algorithm. Step 1: import necessary class of the library from communitytopic import CommunityTopic from communitytopic import PreProcessing Step 2: Load raw corpus as the dataset, here we are using BBC dataset. with open(\"<Path-To-train-Dataset>/bbc_train.txt\", \"r\", encoding='utf-8') as f: bbc_train = f.read() with open(\"<Path-To-Test-Dataset>/bbc_test.txt\", \"r\", encoding='utf-8') as f: bbc_test = f.read() Step 3: Performing pre-processing on training and testing corpus tokenized_bbc_train_sents, tokenized_bbc_train_docs, tokenized_bbc_test_docs, dictionary = PreProcessing.do_preprocessing( train=bbc_train, test=bbc_test, ner=1, pos_filter=3, phrases=\"npmi\", phrase_threshold=0.35, language=\"en\") Step 4: Applying Community Topic algorithm on pre-processed data community_topic = CommunityTopic(train_corpus=tokenized_bbc_train_sents, dictionary=dictionary) community_topic.fit() Step 5: Get topic words founded by abovr algorithm topic_words = community_topic.get_topics_words_topn(10)","title":"Getting Started"},{"location":"installation-guide/","text":"Installation guide System requirement Python >= 3.6 commodity hardware setuptools~=67.6.0 spacy~=3.5.0 numpy~=1.21.5 gensim~=4.2.0 networkx~=2.8.4 igraph~=0.10.4 Installation Option The easy way to install CommunityTopic is: pip install communitytopic Spacy models for different languages for pre-processing (Commuity Topic has a pre-processing function as shown in below getting started example, and it requires spacy model to be dowloaded via python for the language which we are using). Following are commands for the same : Language Commond to download spacy model English !python -m spacy download en_core_web_sm Italian !python -m spacy download it_core_news_sm French !python -m spacy download fr_core_news_sm German !python -m spacy download de_core_news_sm Spanish !python -m spacy download es_core_news_sm","title":"Installation Guide"},{"location":"installation-guide/#installation-guide","text":"System requirement Python >= 3.6 commodity hardware setuptools~=67.6.0 spacy~=3.5.0 numpy~=1.21.5 gensim~=4.2.0 networkx~=2.8.4 igraph~=0.10.4 Installation Option The easy way to install CommunityTopic is: pip install communitytopic Spacy models for different languages for pre-processing (Commuity Topic has a pre-processing function as shown in below getting started example, and it requires spacy model to be dowloaded via python for the language which we are using). Following are commands for the same : Language Commond to download spacy model English !python -m spacy download en_core_web_sm Italian !python -m spacy download it_core_news_sm French !python -m spacy download fr_core_news_sm German !python -m spacy download de_core_news_sm Spanish !python -m spacy download es_core_news_sm","title":"Installation guide"}]}